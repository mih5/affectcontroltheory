p + xlab("Logit scale") + ylab("Logit scale") + ggtitle("Comparison of probit link function with logistic function") + theme(text = element_text(size = 18))
p <- ggplot(dataLong, aes(x=logitscale,y=value, factor = variable, color = variable)) + geom_line() +  geom_vline(xintercept = dataLong$logitscale[c(900)], linetype = "longdash", alpha = 0.5, color = "red")+ geom_text(data=sampleData,aes(x = logitscale, y = -7, label = probs, color = 1)) +theme_bw()
p + xlab("Logit scale") + ylab("Logit scale") + ggtitle("Comparison of probit link function with logistic function") + theme(text = element_text(size = 18))
p <- ggplot(dataLong, aes(x=logitscale,y=value, factor = variable, color = variable)) + geom_line() +  geom_vline(xintercept = dataLong$logitscale[c(900)], linetype = "longdash", alpha = 0.5, color = "red")+ geom_text(data=sampleData,aes(x = logitscale, y = -7, label = probs, color = "black")) +theme_bw()
p + xlab("Logit scale") + ylab("Logit scale") + ggtitle("Comparison of probit link function with logistic function") + theme(text = element_text(size = 18))
p <- ggplot(dataLong, aes(x=logitscale,y=value, factor = variable, color = variable)) + geom_line() +  geom_vline(xintercept = dataLong$logitscale[c(900)], linetype = "longdash", alpha = 0.5, color = "red")+ geom_text(data=sampleData,aes(x = logitscale, y = -7, label = probs)) +theme_bw()
p + xlab("Logit scale") + ylab("Logit scale") + ggtitle("Comparison of probit link function with logistic function") + theme(text = element_text(size = 18))
?melt
dataLong <- melt(data, measure.vars = c("logit","probit"), variable.name = "Link")
sampleData <- dataLong[c(1,10,100,500,900,990,999),]
head(dataLong)
tail(dataLong)
require(ggplot2)
require(dplyr)
p <- ggplot(dataLong, aes(x=logitscale,y=value, factor = Link, color = Link)) + geom_line() +  geom_vline(xintercept = dataLong$logitscale[c(900)], linetype = "longdash", alpha = 0.5, color = "red")+ geom_text(data=sampleData,aes(x = logitscale, y = -7, label = probs)) +theme_bw()
p + xlab("Logit scale") + ylab("Logit scale") + ggtitle("Comparison of probit link function with logistic function") + theme(text = element_text(size = 18))
getTransform <- function(eta){
probScale <- pnorm(eta[1])
return(log(probScale/(1-probScale))/eta[1])
}
getTransform(2)
getTransform(-2)
getTransform(2)*2
getTransform(0)*2
getTransform(0)*0
getTransform(2)*2*2
?dbinom
dbinom(1,4,0.5)
dbinom(2,4,0.5)
dbinom(3,4,0.5)
dbinom(4,4,0.5)
0.5^4
SpeedDating <- read.csv("C:/Users/Mao/Dropbox/Senior Year/Coursera/Speed Dating/SpeedDating.csv")
names(SpeedDating)
mean(SpeedDating$SharedInterestsF)
mean(SpeedDating$SharedInterestsF, na.rm=TRUE)
sd(SpeedDating$SharedInterestsF, na.rm=TRUE)
pnorm(0.63)
1-0.7356527
1-pnorm(6.3)
1-pnorm(1.58)
x <- c(111261,768771,610856,102400,377109)
y <- c(0.745,4.030,3.368,0.700,2.315)
plot(x,y)
plot(x,y, main = "Compression Time vs. Bits in", xlab="Bits in", ylab ="Time (s)")
plot(x,z, main = "Bits out vs. Bits in", xlab="Bits in", ylab ="Bits out", type = "l")
z <- c(73791,439405,369331,73588,247424)
plot(x,z, main = "Bits out vs. Bits in", xlab="Bits in", ylab ="Bits out", type = "l")
plot(x,z, main = "Bits out vs. Bits in", xlab="Bits in", ylab ="Bits out")
plot(x,y, main = "Compression Time vs. Bits in", xlab="Bits in", ylab ="Time (s)")
plot(x,z, main = "Bits out vs. Bits in", xlab="Bits in", ylab ="Bits out")
x <- c(2149096,  3706306, 786568, 1179784, 786568, 1179784, 1498414, 1179784)
y <- c(18.321,  20.690, 7.396 , 10.732, 7.554, 9.711 , 10.619, 10.083)
z <- c(2034591, 2188589,  766142,  1109969 , 756964,  1085497, 1127641,  1135857)
plot(x,z, main = "Bits out vs. Bits in", xlab="Bits in", ylab ="Bits out")
plot(x,y, main = "Compression Time vs. Bits in", xlab="Bits in", ylab ="Time (s)")
library("knitr")
system("pandoc -v")
pandoc('TXF_report_objectives.html', format='latex' )
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=64, glm.family = gaussian(), prior.param = priors)
require(mvtnorm) #for sampling from multivariate normal
require(ggplot2) #for plotting
require(BMA) #Adrian Raftery's Bayesian Model Averaging Package
require(BMS)
require(dplyr) #for transforming data
require(magrittr) #for coding semantics
# DEBUG BMA
# BMA doesn't work with priors specified, why is this the case?
setwd("~/GitHub/affectControlTheory/simulation/Report 1-30")
source("modelCoefs.R")
source("lynnPriors.R")
sim.B <- simulation.model.eae.3way
trainSize = 100
testSize = 100
require(mvtnorm) #for sampling from multivariate normal
require(ggplot2) #for plotting
require(BMA) #Adrian Raftery's Bayesian Model Averaging Package
require(BMS)
require(dplyr) #for transforming data
require(magrittr) #for coding semantics
# GENERATE DATA
#I want the data generated from the true model to have characteristics similar to the duke10 data
duke10 <- read.csv("~/GitHub/affectcontroltheory/Data Merged by Event/duke10_by_event.csv")
#covariate means
X.mean=matrix(colMeans(duke10[,c("ae","ap","aa","be","bp","ba","oe","op","oa")]))
#covariate SD
X.var=var(duke10[,c("ae","ap","aa","be","bp","ba","oe","op","oa")])
#simulated covariates
sim.X1= rmvnorm(trainSize+testSize,X.mean,X.var)
colnames(sim.X1) <- colnames(duke10[,10:18])
# TWO-WAY CROSS WORD INTERACTIONS
sim.X1.interaction = as.matrix(t(apply(sim.X1,1,combn,2,prod)))
colnames(sim.X1.interaction)=paste(combn(names(duke10[,10:18]),2,paste,collapse="."),sep="")
sim.X1.interaction.2=sim.X1.interaction[,c(-1,-2,-9,-22,-23,-27,-34,-35,-36)]
# THREE WAY CROSS WORD INTERACTIONS
#first, generate all three-way interaction names
sim.X1.3.names.full=paste(combn(names(duke10[,10:18]),3,paste,collapse="."),sep="")
#for-loop looks through all the interaction names and stores only those which are cross-word interactions
sim.X1.3.names = matrix(nrow = 84, ncol =2)
for (i in 1:length(sim.X1.3.names.full)){
interaction=sim.X1.3.names.full[i]
words = c(substr(interaction,1,1),substr(interaction,4,4),substr(interaction,7,7))
if(!is.na(sum(pmatch(c("a","b","o"),words)))){
sim.X1.3.names[i,] <- c(interaction,TRUE)
}
else{
sim.X1.3.names[i,] <- c("",FALSE)
}
}
#then, calculate the interactions and use the names to select which ones to keep
sim.X1.interaction.3 <- as.matrix(t(apply(sim.X1,1,combn,3,prod)))[,sim.X1.3.names[,2]=="TRUE"]
colnames(sim.X1.interaction.3)=sim.X1.3.names[which(sim.X1.3.names[,2]=="TRUE"),1]
# COMBINE TWO-WAY AND THREE-WAY INTERACTIONS
sim.X1.design.matrix = (cbind(1,sim.X1,sim.X1.interaction.2,sim.X1.interaction.3))
#let the sd of the random error come from a lm on the real data
model.summary <- lm(eae ~. , data = duke10[,c("eae", "ae","ap","aa","be","bp","ba","oe","op","oa")]) %>% summary()
Y.sd <- model.summary$sigma
#GENERATE FAKE DATA FROM MODEL
#here we add intercept
sim.Y=matrix(rmvnorm(1,sim.X1.design.matrix%*%sim.B,diag(Y.sd^2,trainSize+testSize)),ncol=1)
#minus the intercept
sim.X1.design.dataframe <- data.frame(sim.X1.design.matrix)
sim.data <- data.frame(cbind(eae=sim.Y,sim.X1.design.matrix))
names(sim.data) <- c("response","intercept",names(sim.data)[c(-1,-2)])
####################
# CONDUCT ANALYSIS #
####################
#sample observations for training set
select.training.set <- sample(1:(testSize+trainSize),trainSize)
#train.data, a training set consisting of 150 observations
train.data <- sim.data[select.training.set,]
#test.data, a test set consisting of 850 observations
test.data <- sim.data[-select.training.set,]
# BMA
priors <- lynnPriors[,1]
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=64, glm.family = gaussian(), prior.param = priors)
priors
priors <- lynnPriors[,"eae"]
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=64, glm.family = gaussian(), prior.param = priors)
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=64, glm.family = gaussian(), prior.param = rep(0,63))
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = priors)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=64, glm.family = gaussian(), prior.param = priors)
summary(sim.model.BMA)
?bic.glm
library(MASS)
data(UScrime)
f <- formula(log(y) ~  log(M)+So+log(Ed)+log(Po1)+log(Po2)+log(LF)+
log(M.F)+ log(Pop)+log(NW)+log(U1)+log(U2)+
log(GDP)+log(Ineq)+log(Prob)+log(Time))
glm.out.crime <- bic.glm(f, data = UScrime, glm.family = gaussian())
summary(glm.out.crime)
log(GDP)+log(Ineq)+log(Prob)+log(Time), maxCol = 5)
glm.out.crime <- bic.glm(f, data = UScrime, glm.family = gaussian(), maxCol = 5)
glm.out.crime <- bic.glm(f, data = UScrime, glm.family = gaussian(), maxCol = 5)
glm.out.crime <- bic.glm(f, data = UScrime, glm.family = gaussian(), maxCol = 10)
summary(glm.out.crime)
glm.out.crime <- bic.glm(f, data = UScrime, glm.family = gaussian(), maxCol = 15)
glm.out.crime <- bic.glm(f, data = UScrime, glm.family = gaussian(), maxCol = 16)
summary(glm.out.crime)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0,63))
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), prior.param = rep(0,63))
summary(sim.model.BMA)
3+9
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0,63))
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=64, glm.family = gaussian(), prior.param = priors)
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:12)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), prior.param = rep(0,63))
sim.model.BMA <- bic.glm(x = train.data[,c(3:11)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), prior.param = rep(0,10))
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0.5,63))
sim.model.BMA <- bic.glm(x = train.data[,c(3:11)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), prior.param = rep(0.5,9))
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:11)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), prior.param = rep(0.5,9))
sim.model.BMA <- bic.glm(x = train.data[,c(3:30)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), prior.param = rep(0.5,9))
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:40)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), prior.param = rep(0.5,9))
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:60)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), prior.param = rep(0.5,9))
summary(sim.model.BMA)
length(3:60)
sim.model.BMA <- bic.glm(x = train.data[,c(3:60)], y = train.data[,"response"], maxCol=65, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:60)], y = train.data[,"response"], maxCol=40, glm.family = gaussian())
sim.model.BMA <- bic.glm(x = train.data[,c(3:60)], y = train.data[,"response"], maxCol=10, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:20)], y = train.data[,"response"], maxCol=65, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:30)], y = train.data[,"response"], maxCol=65, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:40)], y = train.data[,"response"], maxCol=65, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:50)], y = train.data[,"response"], maxCol=65, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:60)], y = train.data[,"response"], maxCol=65, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:59)], y = train.data[,"response"], maxCol=65, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:51)], y = train.data[,"response"], maxCol=65, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:51)], y = train.data[,"response"], maxCol=20, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:51)], y = train.data[,"response"], maxCol=20, glm.family = gaussian())
sim.model.BMA <- bic.glm(x = train.data[,c(3:51)], y = train.data[,"response"], maxCol=20, glm.family = gaussian(), occam.window = FALSE)
sim.model.BMA <- bic.glm(x = train.data[,c(3:51)], y = train.data[,"response"], maxCol=20, glm.family = gaussian(), nbest = 1)
sim.model.BMA <- bic.glm(x = train.data[,c(3:51)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), nbest = 1)
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=65, glm.family = gaussian(), nbest = 1)
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(3:52)], y = train.data[,"response"], maxCol=65, glm.family = gaussian())
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=30, glm.family = gaussian(), nbest = 1)
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=30, glm.family = gaussian(), OR.fix = 1)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=30, glm.family = gaussian(), strict = TRUE)
sim.model.BMA <- bic.glm(x = train.data[,c(3:11)], y = train.data[,"response"], maxCol=9, glm.family = gaussian(), prior.param = rep(0.5,9))
dropcols <- function(x, y, wt, maxCols = 31) {
# CF: copied from bicreg (undocumented)
x1.ldf <- data.frame(x, y = y)
temp.wt <- wt
lm.out <- lm(y ~ ., data = x1.ldf, weights = temp.wt)
form.vars <- all.vars(formula(lm.out))[-1]
any.dropped <- FALSE
dropped.which <- NULL
while (length(lm.out$coefficients) > maxCol) {
any.dropped <- TRUE
droplm <- drop1(lm.out, test = "none")
dropped <- row.names(droplm)[which.min(droplm$RSS[-1]) +
1]
dropped.index <- match(dropped, form.vars)
form.vars <- form.vars[-dropped.index]
formla <- formula(paste("y", "~", paste(form.vars,
collapse = " + "), sep = " "))
lm.out <- lm(formla, data = x1.ldf, weights = temp.wt)
dropped.which <- c(dropped.which, dropped)
}
new.var.names <- names(lm.out$coefficients)
return(list(mm = model.matrix(lm.out)[, -1, drop = FALSE],
any.dropped = any.dropped, dropped = dropped.which,
var.names = new.var.names))
}
dropcols(train.data[c(3:11)], train.data[,"response"], maxCols = 10)
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,nrow(x)) maxCols = 10)
dim(trainData)
dim(train.data)
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,100) maxCols = 10)
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,100), maxCols = 10)
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,100), maxCol = 10)
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,100), maxCols = 10)
dropcols
install.packages("BMA")
install.packages("BMA")
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0.5,63))
require(BMA)
# DEBUG BMA
# BMA doesn't work with priors specified, why is this the case?
setwd("~/GitHub/affectControlTheory/simulation/Report 1-30")
source("modelCoefs.R")
source("lynnPriors.R")
sim.B <- simulation.model.eae.3way
trainSize = 100
testSize = 100
require(mvtnorm) #for sampling from multivariate normal
require(ggplot2) #for plotting
require(BMA) #Adrian Raftery's Bayesian Model Averaging Package
require(BMS)
require(dplyr) #for transforming data
require(magrittr) #for coding semantics
# GENERATE DATA
#I want the data generated from the true model to have characteristics similar to the duke10 data
duke10 <- read.csv("~/GitHub/affectcontroltheory/Data Merged by Event/duke10_by_event.csv")
#covariate means
X.mean=matrix(colMeans(duke10[,c("ae","ap","aa","be","bp","ba","oe","op","oa")]))
#covariate SD
X.var=var(duke10[,c("ae","ap","aa","be","bp","ba","oe","op","oa")])
#simulated covariates
sim.X1= rmvnorm(trainSize+testSize,X.mean,X.var)
colnames(sim.X1) <- colnames(duke10[,10:18])
# TWO-WAY CROSS WORD INTERACTIONS
sim.X1.interaction = as.matrix(t(apply(sim.X1,1,combn,2,prod)))
colnames(sim.X1.interaction)=paste(combn(names(duke10[,10:18]),2,paste,collapse="."),sep="")
sim.X1.interaction.2=sim.X1.interaction[,c(-1,-2,-9,-22,-23,-27,-34,-35,-36)]
# THREE WAY CROSS WORD INTERACTIONS
#first, generate all three-way interaction names
sim.X1.3.names.full=paste(combn(names(duke10[,10:18]),3,paste,collapse="."),sep="")
#for-loop looks through all the interaction names and stores only those which are cross-word interactions
sim.X1.3.names = matrix(nrow = 84, ncol =2)
for (i in 1:length(sim.X1.3.names.full)){
interaction=sim.X1.3.names.full[i]
words = c(substr(interaction,1,1),substr(interaction,4,4),substr(interaction,7,7))
if(!is.na(sum(pmatch(c("a","b","o"),words)))){
sim.X1.3.names[i,] <- c(interaction,TRUE)
}
else{
sim.X1.3.names[i,] <- c("",FALSE)
}
}
#then, calculate the interactions and use the names to select which ones to keep
sim.X1.interaction.3 <- as.matrix(t(apply(sim.X1,1,combn,3,prod)))[,sim.X1.3.names[,2]=="TRUE"]
colnames(sim.X1.interaction.3)=sim.X1.3.names[which(sim.X1.3.names[,2]=="TRUE"),1]
# COMBINE TWO-WAY AND THREE-WAY INTERACTIONS
sim.X1.design.matrix = (cbind(1,sim.X1,sim.X1.interaction.2,sim.X1.interaction.3))
#let the sd of the random error come from a lm on the real data
model.summary <- lm(eae ~. , data = duke10[,c("eae", "ae","ap","aa","be","bp","ba","oe","op","oa")]) %>% summary()
Y.sd <- model.summary$sigma
#GENERATE FAKE DATA FROM MODEL
#here we add intercept
sim.Y=matrix(rmvnorm(1,sim.X1.design.matrix%*%sim.B,diag(Y.sd^2,trainSize+testSize)),ncol=1)
#minus the intercept
sim.X1.design.dataframe <- data.frame(sim.X1.design.matrix)
sim.data <- data.frame(cbind(eae=sim.Y,sim.X1.design.matrix))
names(sim.data) <- c("response","intercept",names(sim.data)[c(-1,-2)])
####################
# CONDUCT ANALYSIS #
####################
#sample observations for training set
select.training.set <- sample(1:(testSize+trainSize),trainSize)
#train.data, a training set consisting of 150 observations
train.data <- sim.data[select.training.set,]
#test.data, a test set consisting of 850 observations
test.data <- sim.data[-select.training.set,]
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0.5,63))
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0.5,63))
sim.model.BMA <- bic.glm(x = train.data[,c(3:11)], y = train.data[,"response"], maxCol=9, glm.family = gaussian(), prior.param = rep(0.5,9))
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=64, glm.family = gaussian(), prior.param = priors)
summary(sim.model.BMA)
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,100), maxCols = 10)
maxCol = 10
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,100), maxCols = 10)
maxCol = 9
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,100), maxCols = 10)
maxCol = 5
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,100), maxCols = 10)
maxCol = 12
dropcols(train.data[c(3:11)], train.data[,"response"], wt = rep(1,100), maxCols = 10)
sessionInfo()
# DEBUG BMA
# BMA doesn't work with priors specified, why is this the case?
setwd("~/GitHub/affectControlTheory/simulation/Report 1-30")
source("modelCoefs.R")
source("lynnPriors.R")
sim.B <- simulation.model.eae.3way
trainSize = 100
testSize = 100
require(mvtnorm) #for sampling from multivariate normal
require(ggplot2) #for plotting
require(BMA) #Adrian Raftery's Bayesian Model Averaging Package
require(BMS)
require(dplyr) #for transforming data
require(magrittr) #for coding semantics
# GENERATE DATA
#I want the data generated from the true model to have characteristics similar to the duke10 data
duke10 <- read.csv("~/GitHub/affectcontroltheory/Data Merged by Event/duke10_by_event.csv")
#covariate means
X.mean=matrix(colMeans(duke10[,c("ae","ap","aa","be","bp","ba","oe","op","oa")]))
#covariate SD
X.var=var(duke10[,c("ae","ap","aa","be","bp","ba","oe","op","oa")])
#simulated covariates
sim.X1= rmvnorm(trainSize+testSize,X.mean,X.var)
colnames(sim.X1) <- colnames(duke10[,10:18])
# TWO-WAY CROSS WORD INTERACTIONS
sim.X1.interaction = as.matrix(t(apply(sim.X1,1,combn,2,prod)))
colnames(sim.X1.interaction)=paste(combn(names(duke10[,10:18]),2,paste,collapse="."),sep="")
sim.X1.interaction.2=sim.X1.interaction[,c(-1,-2,-9,-22,-23,-27,-34,-35,-36)]
# THREE WAY CROSS WORD INTERACTIONS
#first, generate all three-way interaction names
sim.X1.3.names.full=paste(combn(names(duke10[,10:18]),3,paste,collapse="."),sep="")
#for-loop looks through all the interaction names and stores only those which are cross-word interactions
sim.X1.3.names = matrix(nrow = 84, ncol =2)
for (i in 1:length(sim.X1.3.names.full)){
interaction=sim.X1.3.names.full[i]
words = c(substr(interaction,1,1),substr(interaction,4,4),substr(interaction,7,7))
if(!is.na(sum(pmatch(c("a","b","o"),words)))){
sim.X1.3.names[i,] <- c(interaction,TRUE)
}
else{
sim.X1.3.names[i,] <- c("",FALSE)
}
}
#then, calculate the interactions and use the names to select which ones to keep
sim.X1.interaction.3 <- as.matrix(t(apply(sim.X1,1,combn,3,prod)))[,sim.X1.3.names[,2]=="TRUE"]
colnames(sim.X1.interaction.3)=sim.X1.3.names[which(sim.X1.3.names[,2]=="TRUE"),1]
# COMBINE TWO-WAY AND THREE-WAY INTERACTIONS
sim.X1.design.matrix = (cbind(1,sim.X1,sim.X1.interaction.2,sim.X1.interaction.3))
#let the sd of the random error come from a lm on the real data
model.summary <- lm(eae ~. , data = duke10[,c("eae", "ae","ap","aa","be","bp","ba","oe","op","oa")]) %>% summary()
Y.sd <- model.summary$sigma
#GENERATE FAKE DATA FROM MODEL
#here we add intercept
sim.Y=matrix(rmvnorm(1,sim.X1.design.matrix%*%sim.B,diag(Y.sd^2,trainSize+testSize)),ncol=1)
#minus the intercept
sim.X1.design.dataframe <- data.frame(sim.X1.design.matrix)
sim.data <- data.frame(cbind(eae=sim.Y,sim.X1.design.matrix))
names(sim.data) <- c("response","intercept",names(sim.data)[c(-1,-2)])
####################
# CONDUCT ANALYSIS #
####################
#sample observations for training set
select.training.set <- sample(1:(testSize+trainSize),trainSize)
#train.data, a training set consisting of 150 observations
train.data <- sim.data[select.training.set,]
#test.data, a test set consisting of 850 observations
test.data <- sim.data[-select.training.set,]
# BMA
priors <- lynnPriors[,"eae"]
# makes no sense, only intercept
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=64, glm.family = gaussian(), prior.param = priors)
summary(sim.model.BMA)
# error with dropcols when maxCol is less than number of predictors
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0.5,63))
bic.glm
glm.out <- glm(y ~ ., family = gaussian,
data = data.frame(y = train.data[,"response"], train.data[,c(-1,-2)]))
?drop1
drop1(glm.out, test = "Chisq")
dropglm <- drop1(glm.out, test = "Chisq"
)
dropglm$LRT[-1]
dropglm$LRT
dropglm <- drop1(glm.out, test = "none")
dropglm$LRT
dropglm <- drop1(glm.out, test = "none")
dropglm$LRT
dropglm <- drop1(glm.out, test = "Chisq")
dropglm$LRT
dropglm$Chisq
?anova
dropglm <- drop1(glm.out, test = "Chisq")
dropglm$LRT
dropglm
dropglm <- drop1(glm.out, test = "Chisq")
dropglm$LRT
names(dropglm)
dropglm[,"Pr(>Chi"]
dropglm[,"Pr(>Chi)"]
?which.max
dropglm <- drop1(glm.out, test = "Chisq")
dropped <- which.max(dropglm[,"Pr(>Chi)"][-1]) + 1
dropped
dropglm <- drop1(glm.out, test = "Chisq")
dropglm
dropped <- which.max(dropglm[,"scaled dev."][-1]) + 1
dropped
dropglm[,"scaled dev."]
dropglm[,"scaled dev."][-1]
dropglm
dropglm <- drop1(glm.out, test = "Chisq")
dropped <- which.max(dropglm[,"scaled dev."][-1]) + 1
dropped
sessionInfo()
?trace
trace(dropcols, sim.model.BMA <- bic.glm(x = train.data[,c(3:11)], y = train.data[,"response"], maxCol=9, glm.family = gaussian(), prior.param = rep(0.5,9)))
?dropcols
?bic.glm.matrix
trace(bic.glm.matrix, sim.model.BMA <- bic.glm(x = train.data[,c(3:11)], y = train.data[,"response"], maxCol=9, glm.family = gaussian(), prior.param = rep(0.5,9)))
trace(bic.glm.matrix)
sim.model.BMA <- bic.glm(x = train.data[,c(3:11)], y = train.data[,"response"], maxCol=9, glm.family = gaussian(), prior.param = rep(0.5,9))
untrace(bic.glm.matrix)
trace(sum)
hist(stats::rnorm(100)) # shows about 3-4 calls to sum()
untrace(sum)
trace(bic.glm.data.frame)
sim.model.BMA <- bic.glm(x = train.data[,c(3:11)], y = train.data[,"response"], maxCol=9, glm.family = gaussian(), prior.param = rep(0.5,9))
untrace(bic.glm.matrix)
?assignInNamespace
source(fixBMA.R)
source("fixBMA.R")
sessionInfo()
BMA
assignInNamespace("bic.glm", bic.glm, "BMA")
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0.5,63))
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0.5,63))
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0.5,63))
Q
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=63, glm.family = gaussian(), prior.param = rep(0.5,63))
source("fixBMA.R")
assignInNamespace("bic.glm", bic.glm, "BMA")
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=30, glm.family = gaussian(), prior.param = rep(0.5,63))
summary(sim.model.BMA)
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=30, glm.family = gaussian(), prior.param = priors)
summary(sim.model.BMA)
var.iden.BMS <- rownames(coef(sim.model.BMS))[which(coef(sim.model.BMS)[,1]>0.5)]
var.iden.BMA <- sim.model.BMA$namesx[sim.model.BMA$probne0>50]
var.iden.BMA
priors
source("fixBMA.R")
assignInNamespace("bic.glm", bic.glm, "BMA")
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=30, glm.family = gaussian(), prior.param = priors)
dropped <- which.max(dropglm$"Pr(Chi)"[-1]) + 1
dropped
dropglm$"Pr(Chi)"[-1]
dropglm$"Pr(Chi)"
dropglm[,"Pr(Chi)]
:
""
q
p9a8usdfpo
;
""
dropped <- which.max(dropglm[,"Pr(Chi)"][-1]) + 1
dropped
names(dropglm)
dropped <- which.max(dropglm[,"Pr(>Chi)"][-1]) + 1
dropped
source("fixBMA.R")
assignInNamespace("bic.glm", bic.glm, "BMA")
sim.model.BMA <- bic.glm(x = train.data[,c(-1,-2)], y = train.data[,"response"], maxCol=30, glm.family = gaussian(), prior.param = priors)
summary(sim.model.BMA)
var.iden.BMA <- sim.model.BMA$namesx[sim.model.BMA$probne0>50]
var.iden.BMA
